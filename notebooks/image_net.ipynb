{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08944a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct as st\n",
    "import numpy as np\n",
    "\n",
    "from net import *\n",
    "from net.util import relu, d_relu, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4926cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Helper function to parse IDX files\n",
    "def parse_idx(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        magic = st.unpack('>I', file.read(4))[0]  # Magic number (4 bytes)\n",
    "        num_items = st.unpack('>I', file.read(4))[0]  # Number of items (4 bytes)\n",
    "\n",
    "        if magic == 2051:  # Magic number for images\n",
    "            num_rows = st.unpack('>I', file.read(4))[0]\n",
    "            num_cols = st.unpack('>I', file.read(4))[0]\n",
    "            num_bytes = num_items * num_rows * num_cols\n",
    "            data = np.frombuffer(file.read(num_bytes), dtype=np.uint8)\n",
    "            return data.reshape(num_items, num_rows, num_cols)\n",
    "        elif magic == 2049:  # Magic number for labels\n",
    "            data = np.frombuffer(file.read(num_items), dtype=np.uint8)\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown magic number: {magic}\")\n",
    "\n",
    "# Parse the training data\n",
    "x_train = parse_idx('../data/DigitData/train-images.idx3-ubyte')\n",
    "y_train = parse_idx('../data/DigitData/train-labels.idx1-ubyte')\n",
    "\n",
    "x_test = parse_idx('../data/DigitData/t10k-images.idx3-ubyte')\n",
    "y_test = parse_idx('../data/DigitData/t10k-labels.idx1-ubyte')\n",
    "\n",
    "# Reshape and scale down\n",
    "p = x_train.shape[1]\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04d5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_mean = np.mean(x_train, axis=0)\n",
    "train_std = np.std(x_train, axis=0) + 1e-12\n",
    "\n",
    "x_train = (x_train - train_mean) / train_std\n",
    "x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, p, p) # Need (n, c, p, p) shape\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, p, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6ffe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network -- Values are Hyperparameters\n",
    "strides = 2\n",
    "c1 = 8\n",
    "size_k1 = 3\n",
    "size_p1 = 2\n",
    "c2 = 16\n",
    "size_k2 = 3\n",
    "size_p2 = 2\n",
    "size_d1 = 128\n",
    "k1 = Tensor(np.random.randn(c1, 1, size_k1, size_k1) * 0.01)\n",
    "C1 = Convolutional(k1)\n",
    "B1 = BatchNorm(c1)\n",
    "A1 = Activation(relu, d_relu)\n",
    "P1 = Pool(size_p1, stride=strides)\n",
    "\n",
    "k2= Tensor(np.random.randn(c2, c1, size_k2, size_k2) * 0.01)\n",
    "C2 = Convolutional(k2)\n",
    "B2 = BatchNorm(c2)\n",
    "A2 = Activation(relu, d_relu)\n",
    "P2 = Pool(size_p2, stride=strides)\n",
    "F = Flatten()\n",
    "inp_d1 = 16 * 7 * 7  # Computed based on output dims after 2 conv+pool layers\n",
    "D1 = Dense(inp_d1, size_d1)\n",
    "A3 = Activation(relu, d_relu)\n",
    "D2 = Dense(size_d1, 10)\n",
    "\n",
    "CNN = Network([C1, B1, A1, P1, C2, B2, A2, P2, F, D1, A3, D2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e91e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "LR = 0.1\n",
    "STEPS = 5000\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2873ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Step: 1: 3.2634399321654444\n",
      "Loss at Step: 101: 0.25796681032435137\n",
      "Loss at Step: 201: 0.35970319081459984\n",
      "Loss at Step: 301: 0.04736156376934284\n",
      "Loss at Step: 401: 0.3781280805437729\n",
      "Loss at Step: 501: 0.10142891819750324\n",
      "Loss at Step: 601: 0.09987757882653142\n",
      "Loss at Step: 701: 0.0776410733430767\n",
      "Loss at Step: 801: 0.00915989446325586\n",
      "Loss at Step: 901: 0.041694478047796364\n",
      "Loss at Step: 1001: 0.07270260066600613\n",
      "Loss at Step: 1101: 0.11846535919759993\n",
      "Loss at Step: 1201: 0.07396307217327408\n",
      "Loss at Step: 1301: 0.24777959661423016\n",
      "Loss at Step: 1401: 0.2695538268265702\n",
      "Loss at Step: 1501: 0.006221386268717489\n",
      "Loss at Step: 1601: 0.20986911709390269\n",
      "Loss at Step: 1701: 0.050403183035608284\n",
      "Loss at Step: 1801: 0.23386978052059051\n",
      "Loss at Step: 1901: 0.020516588106240817\n",
      "Loss at Step: 2001: 0.038786329530170235\n",
      "Loss at Step: 2101: 0.0252266773520183\n",
      "Loss at Step: 2201: 0.03935032581351981\n",
      "Loss at Step: 2301: 0.12112966665938461\n",
      "Loss at Step: 2401: 0.01780252668212791\n",
      "Loss at Step: 2501: 0.006963443659472669\n",
      "Loss at Step: 2601: 0.12341050435885803\n",
      "Loss at Step: 2701: 0.18381995320240063\n",
      "Loss at Step: 2801: 0.03879798825233736\n",
      "Loss at Step: 2901: 0.20922249493568207\n",
      "Loss at Step: 3001: 0.05303270089992932\n",
      "Loss at Step: 3101: 0.057947286383241904\n",
      "Loss at Step: 3201: 0.0010006894307056401\n",
      "Loss at Step: 3301: 0.00423196521510146\n",
      "Loss at Step: 3401: 0.35196444733017335\n",
      "Loss at Step: 3501: 0.011730464838431714\n",
      "Loss at Step: 3601: 0.07196592997043534\n",
      "Loss at Step: 3701: 0.10480898256813197\n",
      "Loss at Step: 3801: 0.033198134397027755\n",
      "Loss at Step: 3901: 0.0051058245176697735\n",
      "Loss at Step: 4001: 0.04745334343634533\n",
      "Loss at Step: 4101: 0.02956358047692701\n",
      "Loss at Step: 4201: 0.06170312327474892\n",
      "Loss at Step: 4301: 0.006375196656689582\n",
      "Loss at Step: 4401: 0.08377389232548824\n",
      "Loss at Step: 4501: 0.035836962461186145\n",
      "Loss at Step: 4601: 0.215872816654934\n",
      "Loss at Step: 4701: 0.00014757250115849703\n",
      "Loss at Step: 4801: 0.00697148546043075\n",
      "Loss at Step: 4901: 0.04558309129720801\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_descent(CNN, cross_entropy, x_train, y_train, STEPS, BATCH_SIZE, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b88274bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05839596207484694\n"
     ]
    }
   ],
   "source": [
    "# Training Loss\n",
    "CNN.set_to_predict()\n",
    "logits = CNN.forward(x_test[:8000])\n",
    "print(cross_entropy(logits.value, y_test[:8000], grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2ec9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98075\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(logits.value, axis=1)\n",
    "print(np.mean(preds == y_test[:8000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
